package frc.robot.subsystems;

import static edu.wpi.first.units.Units.*;

import com.ctre.phoenix6.SignalLogger;
import com.ctre.phoenix6.Utils;
import com.ctre.phoenix6.swerve.SwerveDrivetrainConstants;
import com.ctre.phoenix6.swerve.SwerveModuleConstants;
import com.ctre.phoenix6.swerve.SwerveRequest;
import edu.wpi.first.epilogue.Logged;
import edu.wpi.first.math.Matrix;
import edu.wpi.first.math.Nat;
import edu.wpi.first.math.VecBuilder;
import edu.wpi.first.math.Vector;
import edu.wpi.first.math.geometry.Pose2d;
import edu.wpi.first.math.geometry.Pose3d;
import edu.wpi.first.math.geometry.Rotation2d;
import edu.wpi.first.math.geometry.Transform2d;
import edu.wpi.first.math.geometry.Transform3d;
import edu.wpi.first.math.numbers.N1;
import edu.wpi.first.math.numbers.N3;
import edu.wpi.first.math.numbers.N8;
import edu.wpi.first.math.util.Units;
import edu.wpi.first.units.measure.Angle;
import edu.wpi.first.wpilibj.DriverStation;
import edu.wpi.first.wpilibj.DriverStation.Alliance;
import edu.wpi.first.wpilibj.Notifier;
import edu.wpi.first.wpilibj.RobotController;
import edu.wpi.first.wpilibj.Timer;
import edu.wpi.first.wpilibj.smartdashboard.Field2d;
import edu.wpi.first.wpilibj.smartdashboard.SmartDashboard;
import edu.wpi.first.wpilibj2.command.Command;
import edu.wpi.first.wpilibj2.command.Subsystem;
import edu.wpi.first.wpilibj2.command.sysid.SysIdRoutine;
import frc.robot.Constants.FieldConstants;
import frc.robot.Constants.TurretConstants;
import frc.robot.Constants.VisionConstants;
import frc.robot.generated.TunerConstants.TunerSwerveDrivetrain;
import frc.robot.util.AllianceUtil;
import frc.robot.util.ExtendedVisionSystemSim;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;
import java.util.Optional;
import java.util.function.Supplier;
import org.photonvision.EstimatedRobotPose;
import org.photonvision.PhotonCamera;
import org.photonvision.PhotonPoseEstimator;
import org.photonvision.PhotonPoseEstimator.PoseStrategy;
import org.photonvision.simulation.PhotonCameraSim;
import org.photonvision.simulation.SimCameraProperties;
import org.photonvision.targeting.PhotonPipelineResult;
import org.photonvision.targeting.PhotonTrackedTarget;

/**
 * Class that extends the Phoenix 6 SwerveDrivetrain class and implements Subsystem so it can easily
 * be used in command-based projects.
 *
 * <p>Generated by the 2026 Tuner X Swerve Project Generator
 * https://v6.docs.ctr-electronics.com/en/stable/docs/tuner/tuner-swerve/index.html
 */
public class Swerve extends TunerSwerveDrivetrain implements Subsystem {
  private static final double kSimLoopPeriod = 0.004; // 4 ms
  private Notifier m_simNotifier = null;
  private double m_lastSimTime;

  public static record PoseEstimate(
      Pose3d estimatedPose, double timestamp, Vector<N3> visionStandardDeviation)
      implements Comparable<PoseEstimate> {
    @Override
    public int compareTo(PoseEstimate other) {
      if (timestamp > other.timestamp) {
        return 1;
      } else if (timestamp < other.timestamp) {
        return -1;
      }
      return 0;
    }
  }

  private Field2d field = new Field2d();

  private PhotonCamera arducamLeft = new PhotonCamera(VisionConstants.arducamLeftName);
  private PhotonCamera arducamFront = new PhotonCamera(VisionConstants.arducamFrontName);
  private PhotonCamera arducamRight = new PhotonCamera(VisionConstants.arducamRightName);

  private PhotonPoseEstimator leftPoseEstimator =
      new PhotonPoseEstimator(
          FieldConstants.aprilTagLayout,
          PoseStrategy.MULTI_TAG_PNP_ON_COPROCESSOR,
          VisionConstants.arducamLeftTransform);

  private PhotonPoseEstimator rightPoseEstimator =
      new PhotonPoseEstimator(
          FieldConstants.aprilTagLayout,
          PoseStrategy.MULTI_TAG_PNP_ON_COPROCESSOR,
          VisionConstants.arducamRightTransform);

  private PhotonPoseEstimator frontPoseEstimator =
      new PhotonPoseEstimator(
          FieldConstants.aprilTagLayout,
          PoseStrategy.MULTI_TAG_PNP_ON_COPROCESSOR,
          VisionConstants.arducamFrontTransform);

  private List<PhotonPipelineResult> latestArducamLeftResult;
  private List<PhotonPipelineResult> latestArducamRightResult;
  private List<PhotonPipelineResult> latestArducamFrontResult;

  private Optional<Matrix<N3, N3>> arducamLeftMatrix = Optional.empty();
  private Optional<Matrix<N8, N1>> arducamLeftDistCoeffs = Optional.empty();

  private Optional<Matrix<N3, N3>> arducamRightMatrix = Optional.empty();
  private Optional<Matrix<N8, N1>> arducamRightDistCoeffs = Optional.empty();

  private Optional<Matrix<N3, N3>> arducamFrontMatrix = Optional.empty();
  private Optional<Matrix<N8, N1>> arducamFrontDistCoeffs = Optional.empty();

  private SwerveDriveState stateCache = getState();

  private ExtendedVisionSystemSim visionSim;

  private PhotonCameraSim arducamLeftSim;
  private PhotonCameraSim arducamRightSim;
  private PhotonCameraSim arducamFrontSim;

  @Logged(name = "Detected Targets")
  private List<Pose3d> detectedTargets = new ArrayList<>();

  private List<Integer> detectedAprilTags = new ArrayList<>();

  @Logged(name = "Rejected Poses")
  private List<Pose3d> rejectedPoses = new ArrayList<>();

  private List<PoseEstimate> poseEstimates = new ArrayList<>();

  /* Blue alliance sees forward as 0 degrees (toward red alliance wall) */
  private static final Rotation2d kBlueAlliancePerspectiveRotation = Rotation2d.kZero;
  /* Red alliance sees forward as 180 degrees (toward blue alliance wall) */
  private static final Rotation2d kRedAlliancePerspectiveRotation = Rotation2d.k180deg;
  /* Keep track if we've ever applied the operator perspective before or not */
  private boolean m_hasAppliedOperatorPerspective = false;

  /* Swerve requests to apply during SysId characterization */
  private final SwerveRequest.SysIdSwerveTranslation m_translationCharacterization =
      new SwerveRequest.SysIdSwerveTranslation();
  private final SwerveRequest.SysIdSwerveSteerGains m_steerCharacterization =
      new SwerveRequest.SysIdSwerveSteerGains();
  private final SwerveRequest.SysIdSwerveRotation m_rotationCharacterization =
      new SwerveRequest.SysIdSwerveRotation();

  /* SysId routine for characterizing translation. This is used to find PID gains for the drive motors. */
  private final SysIdRoutine m_sysIdRoutineTranslation =
      new SysIdRoutine(
          new SysIdRoutine.Config(
              null, // Use default ramp rate (1 V/s)
              Volts.of(4), // Reduce dynamic step voltage to 4 V to prevent brownout
              null, // Use default timeout (10 s)
              // Log state with SignalLogger class
              state -> SignalLogger.writeString("SysIdTranslation_State", state.toString())),
          new SysIdRoutine.Mechanism(
              output -> setControl(m_translationCharacterization.withVolts(output)), null, this));

  /* SysId routine for characterizing steer. This is used to find PID gains for the steer motors. */
  private final SysIdRoutine m_sysIdRoutineSteer =
      new SysIdRoutine(
          new SysIdRoutine.Config(
              null, // Use default ramp rate (1 V/s)
              Volts.of(7), // Use dynamic voltage of 7 V
              null, // Use default timeout (10 s)
              // Log state with SignalLogger class
              state -> SignalLogger.writeString("SysIdSteer_State", state.toString())),
          new SysIdRoutine.Mechanism(
              volts -> setControl(m_steerCharacterization.withVolts(volts)), null, this));

  /*
   * SysId routine for characterizing rotation.
   * This is used to find PID gains for the FieldCentricFacingAngle HeadingController.
   * See the documentation of SwerveRequest.SysIdSwerveRotation for info on importing the log to SysId.
   */
  private final SysIdRoutine m_sysIdRoutineRotation =
      new SysIdRoutine(
          new SysIdRoutine.Config(
              /* This is in radians per second², but SysId only supports "volts per second" */
              Volts.of(Math.PI / 6).per(Second),
              /* This is in radians per second, but SysId only supports "volts" */
              Volts.of(Math.PI),
              null, // Use default timeout (10 s)
              // Log state with SignalLogger class
              state -> SignalLogger.writeString("SysIdRotation_State", state.toString())),
          new SysIdRoutine.Mechanism(
              output -> {
                /* output is actually radians per second, but SysId only supports "volts" */
                setControl(m_rotationCharacterization.withRotationalRate(output.in(Volts)));
                /* also log the requested output for SysId */
                SignalLogger.writeDouble("Rotational_Rate", output.in(Volts));
              },
              null,
              this));

  /* The SysId routine to test */
  private SysIdRoutine m_sysIdRoutineToApply = m_sysIdRoutineTranslation;

  /**
   * Constructs a CTRE SwerveDrivetrain using the specified constants.
   *
   * <p>This constructs the underlying hardware devices, so users should not construct the devices
   * themselves. If they need the devices, they can access them through getters in the classes.
   *
   * @param drivetrainConstants Drivetrain-wide constants for the swerve drive
   * @param modules Constants for each specific module
   */
  public Swerve(
      SwerveDrivetrainConstants drivetrainConstants, SwerveModuleConstants<?, ?, ?>... modules) {
    super(drivetrainConstants, modules);
    if (Utils.isSimulation()) {
      startSimThread();
      initVisionSim();
    }
  }

  /**
   * Constructs a CTRE SwerveDrivetrain using the specified constants.
   *
   * <p>This constructs the underlying hardware devices, so users should not construct the devices
   * themselves. If they need the devices, they can access them through getters in the classes.
   *
   * @param drivetrainConstants Drivetrain-wide constants for the swerve drive
   * @param odometryUpdateFrequency The frequency to run the odometry loop. If unspecified or set to
   *     0 Hz, this is 250 Hz on CAN FD, and 100 Hz on CAN 2.0.
   * @param modules Constants for each specific module
   */
  public Swerve(
      SwerveDrivetrainConstants drivetrainConstants,
      double odometryUpdateFrequency,
      SwerveModuleConstants<?, ?, ?>... modules) {
    super(drivetrainConstants, odometryUpdateFrequency, modules);
    if (Utils.isSimulation()) {
      startSimThread();
      initVisionSim();
    }
  }

  /**
   * Constructs a CTRE SwerveDrivetrain using the specified constants.
   *
   * <p>This constructs the underlying hardware devices, so users should not construct the devices
   * themselves. If they need the devices, they can access them through getters in the classes.
   *
   * @param drivetrainConstants Drivetrain-wide constants for the swerve drive
   * @param odometryUpdateFrequency The frequency to run the odometry loop. If unspecified or set to
   *     0 Hz, this is 250 Hz on CAN FD, and 100 Hz on CAN 2.0.
   * @param odometryStandardDeviation The standard deviation for odometry calculation in the form
   *     [x, y, theta]ᵀ, with units in meters and radians
   * @param visionStandardDeviation The standard deviation for vision calculation in the form [x, y,
   *     theta]ᵀ, with units in meters and radians
   * @param modules Constants for each specific module
   */
  public Swerve(
      SwerveDrivetrainConstants drivetrainConstants,
      double odometryUpdateFrequency,
      Matrix<N3, N1> odometryStandardDeviation,
      Matrix<N3, N1> visionStandardDeviation,
      SwerveModuleConstants<?, ?, ?>... modules) {
    super(
        drivetrainConstants,
        odometryUpdateFrequency,
        odometryStandardDeviation,
        visionStandardDeviation,
        modules);
    if (Utils.isSimulation()) {
      startSimThread();
      initVisionSim();
    }
  }

  /**
   * Returns a command that applies the specified control request to this swerve drivetrain.
   *
   * @param request Function returning the request to apply
   * @return Command to run
   */
  public Command applyRequest(Supplier<SwerveRequest> request) {
    return run(() -> this.setControl(request.get()));
  }

  /**
   * Runs the SysId Quasistatic test in the given direction for the routine specified by {@link
   * #m_sysIdRoutineToApply}.
   *
   * @param direction Direction of the SysId Quasistatic test
   * @return Command to run
   */
  public Command sysIdQuasistatic(SysIdRoutine.Direction direction) {
    return m_sysIdRoutineToApply.quasistatic(direction);
  }

  /**
   * Runs the SysId Dynamic test in the given direction for the routine specified by {@link
   * #m_sysIdRoutineToApply}.
   *
   * @param direction Direction of the SysId Dynamic test
   * @return Command to run
   */
  public Command sysIdDynamic(SysIdRoutine.Direction direction) {
    return m_sysIdRoutineToApply.dynamic(direction);
  }

  double ctreToFpgaTime(double timestamp) {
    return (Timer.getFPGATimestamp() - Utils.getCurrentTimeSeconds()) + timestamp;
  }

  @Override
  public void periodic() {

    stateCache = getState();

    latestArducamLeftResult = arducamLeft.getAllUnreadResults();
    latestArducamRightResult = arducamRight.getAllUnreadResults();
    latestArducamFrontResult = arducamFront.getAllUnreadResults();

    double stateTimestamp = ctreToFpgaTime(stateCache.Timestamp);

    leftPoseEstimator.addHeadingData(stateTimestamp, stateCache.Pose.getRotation());
    rightPoseEstimator.addHeadingData(stateTimestamp, stateCache.Pose.getRotation());
    frontPoseEstimator.addHeadingData(stateTimestamp, stateCache.Pose.getRotation());

    if (arducamLeftMatrix.isEmpty()) {
      arducamLeftMatrix = arducamLeft.getCameraMatrix();
    }
    if (arducamLeftDistCoeffs.isEmpty()) {
      arducamLeftDistCoeffs = arducamLeft.getDistCoeffs();
    }

    if (arducamRightMatrix.isEmpty()) {
      arducamRightMatrix = arducamRight.getCameraMatrix();
    }
    if (arducamRightDistCoeffs.isEmpty()) {
      arducamRightDistCoeffs = arducamRight.getDistCoeffs();
    }

    if (arducamFrontMatrix.isEmpty()) {
      arducamFrontMatrix = arducamFront.getCameraMatrix();
    }
    if (arducamFrontDistCoeffs.isEmpty()) {
      arducamFrontDistCoeffs = arducamFront.getDistCoeffs();
    }
    updateVisionPoseEstimates();

    field.setRobotPose(stateCache.Pose);

    if (!m_hasAppliedOperatorPerspective || DriverStation.isDisabled()) {
      DriverStation.getAlliance()
          .ifPresent(
              allianceColor -> {
                setOperatorPerspectiveForward(
                    allianceColor == Alliance.Red
                        ? kRedAlliancePerspectiveRotation
                        : kBlueAlliancePerspectiveRotation);
                m_hasAppliedOperatorPerspective = true;
              });
    }
  }

  @Override
  public void simulationPeriodic() {
    // Update camera simulation
    Pose2d robotPose = stateCache.Pose;

    field.getObject("EstimatedRobot").setPose(robotPose);

    visionSim.addFieldObject(
        "Turret",
        stateCache.Pose.transformBy(
            new Transform2d(
                TurretConstants.turretOnRobot.toTranslation2d(),
                simFaceTowards(AllianceUtil.getHubPose(), stateCache.Pose))));

    visionSim.update(robotPose);
  }

  private void startSimThread() {
    m_lastSimTime = Utils.getCurrentTimeSeconds();

    /* Run simulation at a faster rate so PID gains behave more reasonably */
    m_simNotifier =
        new Notifier(
            () -> {
              final double currentTime = Utils.getCurrentTimeSeconds();
              double deltaTime = currentTime - m_lastSimTime;
              m_lastSimTime = currentTime;

              /* use the measured time delta, get battery voltage from WPILib */
              updateSimState(deltaTime, RobotController.getBatteryVoltage());
            });
    m_simNotifier.startPeriodic(kSimLoopPeriod);
  }

  /**
   * Adds a vision measurement to the Kalman Filter. This will correct the odometry pose estimate
   * while still accounting for measurement noise.
   *
   * @param visionRobotPoseMeters The pose of the robot as measured by the vision camera.
   * @param timestampSeconds The timestamp of the vision measurement in seconds.
   */
  @Override
  public void addVisionMeasurement(Pose2d visionRobotPoseMeters, double timestampSeconds) {
    super.addVisionMeasurement(visionRobotPoseMeters, Utils.fpgaToCurrentTime(timestampSeconds));
  }

  /**
   * Adds a vision measurement to the Kalman Filter. This will correct the odometry pose estimate
   * while still accounting for measurement noise.
   *
   * <p>Note that the vision measurement standard deviations passed into this method will continue
   * to apply to future measurements until a subsequent call to {@link
   * #setVisionMeasurementStdDevs(Matrix)} or this method.
   *
   * @param visionRobotPoseMeters The pose of the robot as measured by the vision camera.
   * @param timestampSeconds The timestamp of the vision measurement in seconds.
   * @param visionMeasurementStdDevs Standard deviations of the vision pose measurement in the form
   *     [x, y, theta]ᵀ, with units in meters and radians.
   */
  @Override
  public void addVisionMeasurement(
      Pose2d visionRobotPoseMeters,
      double timestampSeconds,
      Matrix<N3, N1> visionMeasurementStdDevs) {
    super.addVisionMeasurement(
        visionRobotPoseMeters, Utils.fpgaToCurrentTime(timestampSeconds), visionMeasurementStdDevs);
  }

  /**
   * Return the pose at a given timestamp, if the buffer is not empty.
   *
   * @param timestampSeconds The timestamp of the pose in seconds.
   * @return The pose at the given timestamp (or Optional.empty() if the buffer is empty).
   */
  @Override
  public Optional<Pose2d> samplePoseAt(double timestampSeconds) {
    return super.samplePoseAt(Utils.fpgaToCurrentTime(timestampSeconds));
  }

  private void updateVisionPoses(
      List<PhotonPipelineResult> latestResults,
      PhotonPoseEstimator poseEstimator,
      Optional<Matrix<N3, N3>> cameraMatrix,
      Optional<Matrix<N8, N1>> distCoeffs,
      Transform3d cameraTransform,
      double baseSingleTagStdDev,
      double baseMultiTagStdDev,
      double cameraWeight) {

    if (latestResults.isEmpty()) {
      return;
    }

    for (PhotonPipelineResult result : latestResults) {
      Optional<EstimatedRobotPose> optionalVisionPose =
          poseEstimator.estimateCoprocMultiTagPose(result);
      if (optionalVisionPose.isEmpty()) {
        continue;
      }

      EstimatedRobotPose visionPose = optionalVisionPose.get();

      double totalDistance = 0.0;
      int tagCount = 0;

      for (PhotonTrackedTarget target : visionPose.targetsUsed) {
        tagCount++;
        totalDistance +=
            target.getBestCameraToTarget().getTranslation().toTranslation2d().getNorm();

        int apriltagID = target.getFiducialId();
        Optional<Pose3d> tagPose = FieldConstants.aprilTagLayout.getTagPose(apriltagID);

        if (tagPose.isEmpty()) {
          continue;
        }

        detectedAprilTags.add(apriltagID);
        detectedTargets.add(tagPose.get());
      }

      double averageDistance = totalDistance / tagCount;

      if (tagCount > 1 && visionPose.strategy == PoseStrategy.MULTI_TAG_PNP_ON_COPROCESSOR) {
        if (!isValidMultitagPose(
            visionPose.estimatedPose,
            averageDistance,
            visionPose.targetsUsed.size(),
            visionPose.timestampSeconds)) {
          rejectedPoses.add(visionPose.estimatedPose);
          continue;
        }
        poseEstimates.add(
            new PoseEstimate(
                visionPose.estimatedPose,
                visionPose.timestampSeconds,
                getVisionStdDevs(
                    tagCount, averageDistance, baseMultiTagStdDev * (1 / cameraWeight))));
      } else {
        if (!isValidSingleTagPose(visionPose.estimatedPose, averageDistance)) {
          rejectedPoses.add(visionPose.estimatedPose);
          continue;
        }

        poseEstimates.add(
            new PoseEstimate(
                visionPose.estimatedPose,
                visionPose.timestampSeconds,
                getVisionStdDevs(
                    tagCount, averageDistance, baseSingleTagStdDev * (1 / cameraWeight))));
      }
    }
  }

  private void updateVisionPoseEstimates() {
    poseEstimates.clear();
    detectedTargets.clear();
    rejectedPoses.clear();

    updateVisionPoses(
        latestArducamLeftResult,
        leftPoseEstimator,
        arducamLeftMatrix,
        arducamLeftDistCoeffs,
        VisionConstants.arducamLeftTransform,
        Units.inchesToMeters(3.0),
        Units.inchesToMeters(2.5),
        1);

    updateVisionPoses(
        latestArducamRightResult,
        rightPoseEstimator,
        arducamRightMatrix,
        arducamRightDistCoeffs,
        VisionConstants.arducamRightTransform,
        Units.inchesToMeters(3.0),
        Units.inchesToMeters(2.5),
        1);

    updateVisionPoses(
        latestArducamFrontResult,
        frontPoseEstimator,
        arducamFrontMatrix,
        arducamFrontDistCoeffs,
        VisionConstants.arducamFrontTransform,
        Units.inchesToMeters(3.0),
        Units.inchesToMeters(2.5),
        1);

    Collections.sort(poseEstimates);

    for (PoseEstimate poseEstimate : poseEstimates) {
      addVisionMeasurement(
          poseEstimate.estimatedPose().toPose2d(),
          Utils.currentTimeToFPGATime(poseEstimate.timestamp()),
          poseEstimate.visionStandardDeviation());
    }
  }

  private Vector<N3> getVisionStdDevs(
      int tagCount, double averageDistance, double baseStandardDev) {
    double stdDevScale = 1 + (averageDistance * averageDistance) / 30;

    return VecBuilder.fill(
        baseStandardDev * stdDevScale, baseStandardDev * stdDevScale, Double.POSITIVE_INFINITY);
  }

  private boolean isOutOfBounds(Pose3d visionPose) {
    final double fieldTolerance = Units.inchesToMeters(2.5);

    return visionPose.getX() < -fieldTolerance
        || visionPose.getX() > FieldConstants.aprilTagLayout.getFieldLength() + fieldTolerance
        || visionPose.getY() < -fieldTolerance
        || visionPose.getY() > FieldConstants.aprilTagLayout.getFieldWidth() + fieldTolerance
        || visionPose.getZ() < -0.5
        || visionPose.getZ() > 1.6;
  }

  private boolean isValidSingleTagPose(Pose3d visionPose, double distance) {
    if (distance > 4.5) {
      return false;
    }
    // if (DriverStation.isAutonomous()) {
    //   return false;
    // }

    if (isOutOfBounds(visionPose)) {
      return false;
    }

    return true;
  }

  private boolean isValidMultitagPose(
      Pose3d visionPose, double averageDistance, int detectedTargets, double timestampSeconds) {
    if (averageDistance > 4.5) {
      return false;
    }
    // if (DriverStation.isAutonomous()) {
    //   return false;
    // }
    if (isOutOfBounds(visionPose)) {
      return false;
    }

    Optional<Rotation2d> rotationAtTime =
        samplePoseAt(Utils.fpgaToCurrentTime(timestampSeconds)).map((pose) -> pose.getRotation());

    if (rotationAtTime.isEmpty()) {
      return false;
    }

    Rotation2d angleDifference =
        rotationAtTime.get().minus(visionPose.getRotation().toRotation2d());

    double angleTolerance = DriverStation.isAutonomous() ? 8.0 : 15.0;

    if (Math.abs(angleDifference.getDegrees()) > angleTolerance) {
      return false;
    }

    return true;
  }

  private void initVisionSim() {
    SmartDashboard.putData("Swerve/Field", field);
    visionSim = new ExtendedVisionSystemSim("main");

    visionSim.addAprilTags(FieldConstants.aprilTagLayout);

    Matrix<N3, N3> calibError =
        new Matrix<>(
            Nat.N3(),
            Nat.N3(),
            new double[] {
              689.4460449566128,
              0,
              441.7426355934763,
              0,
              688.5536260717645,
              292.82342214293885,
              0,
              0,
              1
            });
    Vector<N8> distCoefficients =
        VecBuilder.fill(
            0.03728225626399143,
            -0.022115127374557862,
            0.0001637647682633715,
            0.0003334141823199474,
            -0.03915318108680384,
            -0.0015121698705335032,
            0.0009546599698249316,
            0.0016260200999396255);

    SimCameraProperties arducamProperties =
        new SimCameraProperties()
            .setCalibration(800, 600, calibError, distCoefficients)
            .setCalibError(0.21, 0.10)
            .setFPS(28)
            .setAvgLatencyMs(36)
            .setLatencyStdDevMs(15)
            .setExposureTimeMs(45);

    arducamLeftSim = new PhotonCameraSim(arducamLeft, arducamProperties);
    arducamRightSim = new PhotonCameraSim(arducamRight, arducamProperties);
    arducamFrontSim = new PhotonCameraSim(arducamFront, arducamProperties);

    visionSim.addCamera(arducamLeftSim, VisionConstants.arducamLeftTransform);
    visionSim.addCamera(arducamRightSim, VisionConstants.arducamRightTransform);
    visionSim.addCamera(arducamFrontSim, VisionConstants.arducamFrontTransform);

    arducamLeftSim.enableRawStream(true);
    arducamLeftSim.enableProcessedStream(true);

    arducamRightSim.enableRawStream(true);
    arducamRightSim.enableProcessedStream(true);

    arducamFrontSim.enableRawStream(true);
    arducamFrontSim.enableProcessedStream(true);
  }

    private Rotation2d simFaceTowards(Pose2d target, Pose2d robotPose) {
    Pose2d turretPose =
        robotPose.transformBy(
            new Transform2d(TurretConstants.turretOnRobot.toTranslation2d(), Rotation2d.kZero));

    Rotation2d turretAngle = target.getTranslation().minus(turretPose.getTranslation()).getAngle();
    Rotation2d angleToFace = turretAngle.minus(robotPose.getRotation());
    Angle testAngle = Degrees.of(angleToFace.getDegrees());

    return Rotation2d.fromDegrees(testAngle.in(Degrees));
  }

  public Pose2d getRobotPose() {
    return stateCache.Pose;
  }


}
